# -*- coding: utf-8 -*-
"""Credit Card Fraud Detection .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19PlowXFEwVP2t-6gPThH5rlkrntMUdnB
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Load the training and testing files
df_train=pd.read_csv('/content/train.csv')
df_test=pd.read_csv('/content/test (3).csv')

# Merge the dataframes
df = pd.concat([df_train, df_test], ignore_index=True)

print(df)

df.shape



df.drop(['ID','Job title','Has a mobile phone','Has a work phone','Has a phone'], axis=1, inplace=True)

df['Is high risk'].value_counts(normalize=True) * 100

df.isnull().sum()

import seaborn as sns
import pandas as pd

# Select only numeric columns for correlation calculation
numeric_df = df.select_dtypes(include=['number'])

# Calculate the correlation matrix using only numeric columns
correlation_matrix = numeric_df.corr()

# Create the heatmap
sns.heatmap(correlation_matrix, annot=True, cmap="viridis")

df['Gender_encoded'] = pd.factorize(df['Gender'])[0]
df['Has a car_encoded'] = pd.factorize(df['Has a car'])[0]
df['Has a property_encoded'] = pd.factorize(df['Has a property'])[0]

df.drop(['Gender','Has a car','Has a property'], axis=1, inplace=True)

df.head()

# Calculate correlation between 'Marital status' and 'Dwelling'
# Convert 'Marital status' and 'Dwelling' to numerical representation using pd.factorize
# before calculating the correlation

# Convert 'Marital status' to numerical representation
df['Marital status_encoded'] = pd.factorize(df['Marital status'])[0]

# Convert 'Dwelling' to numerical representation if it's not already numerical
# Assuming 'Dwelling' might also be categorical
df['Dwelling_encoded'] = pd.factorize(df['Dwelling'])[0]

# Calculate correlation using the encoded columns
correlation = df['Marital status_encoded'].corr(df['Dwelling_encoded'])
print(correlation)

df.drop(['Marital status','Dwelling'], axis=1, inplace=True)

# Convert 'Employment status' and 'Education level' to numerical representations
# using pd.factorize or other encoding methods before calculating correlation.

# Example using pd.factorize:
df['Employment status_encoded'] = pd.factorize(df['Employment status'])[0]
df['Education level_encoded'] = pd.factorize(df['Education level'])[0]

# Now calculate the correlation using the encoded columns:
correlation = df['Employment status_encoded'].corr(df['Education level_encoded'])
print(correlation)

df.drop(['Employment status','Education level'], axis=1, inplace=True)

df

df.drop(['Employment length','Children count'], axis=1, inplace=True)

df

# Plot correlation heatmap
import seaborn as sns
plt.figure(figsize=(14, 10))
sns.heatmap(df.corr(method='pearson', numeric_only=True),
            vmin=-1, vmax=1, annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap', fontsize=14, weight='bold');

# Isolate predictor variables
X= df.drop(columns = ['Is high risk'])

# Isolate target variable
y= df['Is high risk']

# Perform the train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

# Display `X_train`
X_train.head()

# Import the necessary class from scikit-learn
from sklearn.preprocessing import MinMaxScaler

# Normalize numerical features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Import the necessary class from scikit-learn
from sklearn.preprocessing import MinMaxScaler, StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# Transform testing data using the same scaler
X_test_scaled = scaler.transform(X_test)

# Import the necessary class from imbalanced-learn
!pip install imblearn
from imblearn.over_sampling import SMOTE

smote = SMOTE(sampling_strategy='auto')
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

# Fit the model on X_train and y_train
model = LogisticRegression(penalty=None, max_iter=400) # Changed 'none' to None
model.fit(X_train_resampled, y_train_resampled)

# Generate predictions on X_test
y_pred = model.predict(X_test)
y_pred

# Classification report
print(classification_report(y_test, y_pred))

import pandas as pd
from sklearn.decomposition import PCA

# Load your dataset
df = pd.read_csv("your_data.csv")

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df)

# Apply PCA
pca = PCA(n_components=2)  # Choose the desired number of components
X_pca = pca.fit_transform(X_scaled)

# Explained variance ratio
explained_variance = pca.explained_variance_ratio_
print("Explained variance ratio:", explained_variance)

# Visualize the results (if desired)
import matplotlib.pyplot as plt

plt.scatter(X_pca[:, 0], X_pca[:, 1])
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("PCA Visualization")
plt.show()